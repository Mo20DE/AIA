{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ade700e-1f9a-4268-a809-4f829ce411c8",
   "metadata": {},
   "source": [
    "# Exercise 5: Scene-Dependent Image Segmentation\n",
    "\n",
    "The goal of this homework is to implement a model that seperates foreground and background objects for a specific scene.  \n",
    "We will use the highway scene from the Change Detection dataset:  \n",
    "http://jacarini.dinf.usherbrooke.ca/dataset2014#\n",
    "\n",
    "![input image](highway/input/in001600.jpg \"Title\") ![gt image](highway/groundtruth/gt001600.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b5923-78bb-48e7-8db6-79b35b79b942",
   "metadata": {},
   "source": [
    "## Task 1: Create a custom (Pytorch) dataset\n",
    "\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "You need to create a class that inherets from **from torch.utils.data.Dataset** and implements two methods:\n",
    "- **def \\_\\_len\\_\\_(self)**:  returns the length of the dataset\n",
    "- **def \\_\\_getitem\\_\\_(self, idx)**: given an integer idx returns the data x,y\n",
    "    - x is the image as a float tensor of shape: $(3,H,W)$ \n",
    "    - y is the label image as a mask of shape: $(H,W)$ each pixel should contain the label 0 (background) or 1 (foreground). It is recommended to use the type torch.long\n",
    "    \n",
    "**Tips**:\n",
    "- The first 470 images are not labeled. Just ignore these images. \n",
    "- If possible load all images into memory or even directly to GPU to increase speed.\n",
    "- You can change the resolution to fit your model or your memory\n",
    "- Add data augmentation to increase the data size and model robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce6bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd0c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 60.,  29.,  27.,  ...,  82.,  76.,  65.],\n",
      "         [ 46.,  38.,  23.,  ..., 145., 137., 129.],\n",
      "         [ 24.,  40.,  19.,  ..., 189., 189., 191.],\n",
      "         ...,\n",
      "         [255., 255., 255.,  ..., 103., 108., 126.],\n",
      "         [250., 251., 252.,  ..., 105., 112., 115.],\n",
      "         [252., 254., 255.,  ..., 110., 119., 109.]],\n",
      "\n",
      "        [[ 68.,  37.,  35.,  ...,  77.,  71.,  60.],\n",
      "         [ 54.,  46.,  31.,  ..., 140., 132., 124.],\n",
      "         [ 32.,  48.,  27.,  ..., 184., 184., 186.],\n",
      "         ...,\n",
      "         [255., 255., 255.,  ...,  88.,  93., 111.],\n",
      "         [250., 251., 252.,  ...,  92.,  99., 102.],\n",
      "         [252., 254., 255.,  ...,  97., 106.,  96.]],\n",
      "\n",
      "        [[ 53.,  22.,  20.,  ...,  81.,  77.,  66.],\n",
      "         [ 39.,  31.,  16.,  ..., 144., 138., 130.],\n",
      "         [ 17.,  33.,  12.,  ..., 188., 190., 192.],\n",
      "         ...,\n",
      "         [255., 255., 255.,  ...,  81.,  86., 104.],\n",
      "         [250., 251., 252.,  ...,  84.,  91.,  94.],\n",
      "         [252., 254., 255.,  ...,  89.,  98.,  88.]]])\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "img_paths = sorted(glob.glob(\"highway/input/*.jpg\"))\n",
    "imgs = [read_image(img_path).float() for img_path in img_paths]\n",
    "img_label_paths = sorted(glob.glob(\"highway/groundtruth/*.png\"))\n",
    "img_labels = [read_image(img_label_path).bool().int().long().squeeze() for img_label_path in img_label_paths]\n",
    "\n",
    "# split dataset (80/20)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(imgs, img_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "class ChangeDetectionDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e120eb7-0349-4bb0-b305-23a0f3bb5e26",
   "metadata": {},
   "source": [
    "## Task 2: Create a custom Segmentation Model\n",
    "\n",
    "- input: a batch of images $(B,3,H,W)$ \n",
    "- output: a batch of pixel-wise class predictions $(B,C,H,W)$, where $C=2$\n",
    "\n",
    "Tips:\n",
    "- It is recommended to use a Fully-Convolutional Neural Network, because it flexible to the input and output resolution.\n",
    "- Use Residual Blocks with convolutional layers.\n",
    "- Base your model on established segmentation models:\n",
    "    - U-Net: https://arxiv.org/abs/1505.04597\n",
    "    - Deeplab: https://arxiv.org/abs/1606.00915"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85364b0-7010-4710-9373-94a89db8d7c5",
   "metadata": {},
   "source": [
    "## Task 3: Create a training loop\n",
    "- split data into training and test data, e.g. 80% training data and 20% test data using your custom dataset.\n",
    "- Create a Dataloader for your custom datasets \n",
    "- Define a training loop for a single epoch:\n",
    "    - forward pass\n",
    "    - Loss function, e.g. cross entropy\n",
    "    - optimizer \n",
    "    - backward pass\n",
    "    - logging\n",
    "- Define validation loop:\n",
    "    - forward pass\n",
    "    - extract binary labels, e.g. threshold or argmax for each pixel.\n",
    "    - compute evaluation metrics: Accuracy, Precision, Recall and Intersection over Union for each image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e603ff-a32e-4d90-84d9-eba8570d6e63",
   "metadata": {},
   "source": [
    "## Task 4: Small Report of your model and training\n",
    "- visualize training and test error over each epoch\n",
    "- report the evaluation metrics of the final model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
