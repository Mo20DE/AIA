{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ade700e-1f9a-4268-a809-4f829ce411c8",
   "metadata": {},
   "source": [
    "# Exercise 5: Scene-Dependent Image Segmentation\n",
    "\n",
    "The goal of this homework is to implement a model that seperates foreground and background objects for a specific scene.  \n",
    "We will use the highway scene from the Change Detection dataset:  \n",
    "http://jacarini.dinf.usherbrooke.ca/dataset2014#\n",
    "\n",
    "![input image](highway/input/in001600.jpg \"Title\") ![gt image](highway/groundtruth/gt001600.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b5923-78bb-48e7-8db6-79b35b79b942",
   "metadata": {},
   "source": [
    "## Task 1: Create a custom (Pytorch) dataset\n",
    "\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "You need to create a class that inherets from **from torch.utils.data.Dataset** and implements two methods:\n",
    "- **def \\_\\_len\\_\\_(self)**:  returns the length of the dataset\n",
    "- **def \\_\\_getitem\\_\\_(self, idx)**: given an integer idx returns the data x,y\n",
    "    - x is the image as a float tensor of shape: $(3,H,W)$ \n",
    "    - y is the label image as a mask of shape: $(H,W)$ each pixel should contain the label 0 (background) or 1 (foreground). It is recommended to use the type torch.long\n",
    "    \n",
    "**Tips**:\n",
    "- The first 470 images are not labeled. Just ignore these images. \n",
    "- If possible load all images into memory or even directly to GPU to increase speed.\n",
    "- You can change the resolution to fit your model or your memory\n",
    "- Add data augmentation to increase the data size and model robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ce6bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8dd0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeDetectionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if not transform:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((240, 240)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        \n",
    "        self.imgs = sorted(glob.glob(root_dir + \"input/*.jpg\"))\n",
    "        self.imgs = [self.transform(Image.open(img)) for img in self.imgs]\n",
    "        self.gts = sorted(glob.glob(root_dir + \"groundtruth/*.png\"))\n",
    "        self.gts = [(self.transform(Image.open(gt)) > 0).squeeze(dim=0).long() for gt in self.gts]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        gt = self.gts[idx]\n",
    "        return img, gt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "834795ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 246)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "root_dir = \"highway/\"\n",
    "complete_dataset = ChangeDetectionDataset(root_dir)\n",
    "# split dataset (80/20)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "X_train, X_test = random_split(complete_dataset, [0.8, 0.2], generator=generator)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e120eb7-0349-4bb0-b305-23a0f3bb5e26",
   "metadata": {},
   "source": [
    "## Task 2: Create a custom Segmentation Model\n",
    "\n",
    "- input: a batch of images $(B,3,H,W)$ \n",
    "- output: a batch of pixel-wise class predictions $(B,C,H,W)$, where $C=2$\n",
    "\n",
    "Tips:\n",
    "- It is recommended to use a Fully-Convolutional Neural Network, because it flexible to the input and output resolution.\n",
    "- Use Residual Blocks with convolutional layers.\n",
    "- Base your model on established segmentation models:\n",
    "    - U-Net: https://arxiv.org/abs/1505.04597\n",
    "    - Deeplab: https://arxiv.org/abs/1606.00915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a91a8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        in_channels, \n",
    "        out_channels, \n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride, \n",
    "            padding=padding\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if batch_norm else None\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.conv(x)) if self.bn else self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "def createConvSequential(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=1,\n",
    "    batch_norm=True\n",
    "):\n",
    "    conv_sequential = nn.Sequential(\n",
    "        ConvLayer(in_channels, out_channels, kernel_size, stride, padding, batch_norm),\n",
    "        ConvLayer(out_channels, out_channels, kernel_size, stride, padding, batch_norm)\n",
    "    )\n",
    "    return conv_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "891fa30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        # used channels\n",
    "        self.ch1 = 64\n",
    "        self.ch2 = 128\n",
    "        self.ch3 = 256\n",
    "        self.ch4 = 512\n",
    "        self.ch5 = 1024\n",
    "        # downsampling conv-layers\n",
    "        self.ds1 = createConvSequential(self.in_channels, self.ch1)\n",
    "        self.ds2 = createConvSequential(self.ch1, self.ch2)\n",
    "        self.ds3 = createConvSequential(self.ch2, self.ch3)\n",
    "        self.ds4 = createConvSequential(self.ch3, self.ch4)\n",
    "        # bottleneck\n",
    "        self.ds5 = createConvSequential(self.ch4, self.ch5)\n",
    "        # upsampling conv-layers\n",
    "        self.us1 = createConvSequential(self.ch5, self.ch4)\n",
    "        self.us2 = createConvSequential(self.ch4, self.ch3)\n",
    "        self.us3 = createConvSequential(self.ch3, self.ch2)\n",
    "        self.us4 = createConvSequential(self.ch2, self.ch1)\n",
    "        # modules for chanaging channel sizes\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.trans_conv1 = nn.ConvTranspose2d(self.ch5, self.ch4, kernel_size=2, stride=2)\n",
    "        self.trans_conv2 = nn.ConvTranspose2d(self.ch4, self.ch3, kernel_size=2, stride=2)\n",
    "        self.trans_conv3 = nn.ConvTranspose2d(self.ch3, self.ch2, kernel_size=2, stride=2)\n",
    "        self.trans_conv4 = nn.ConvTranspose2d(self.ch2, self.ch1, kernel_size=2, stride=2)\n",
    "        self.last_conv = nn.Conv2d(self.ch1, self.num_classes, kernel_size=1) # creates fore- and background masks\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # downsampling path\n",
    "        ds_res1 = self.ds1(x)\n",
    "        x = self.max_pool(ds_res1)\n",
    "\n",
    "        ds_res2 = self.ds2(x)\n",
    "        x = self.max_pool(ds_res2)\n",
    "\n",
    "        ds_res3 = self.ds3(x)\n",
    "        x = self.max_pool(ds_res3)\n",
    "\n",
    "        ds_res4 = self.ds4(x)\n",
    "        x = self.max_pool(ds_res4)\n",
    "\n",
    "        # bottleneck\n",
    "        x = self.ds5(x)\n",
    "\n",
    "        # upsampling path\n",
    "        x = self.trans_conv1(x)\n",
    "        x = torch.concatenate((ds_res4, x), dim=1)\n",
    "        x = self.us1(x)\n",
    "\n",
    "        x = self.trans_conv2(x)\n",
    "        x = torch.concatenate((ds_res3, x), dim=1)\n",
    "        x = self.us2(x)\n",
    "\n",
    "        x = self.trans_conv3(x)\n",
    "        x = torch.concatenate((ds_res2, x), dim=1)\n",
    "        x = self.us3(x)\n",
    "\n",
    "        x = self.trans_conv4(x)\n",
    "        x = torch.concatenate((ds_res1, x), dim=1)\n",
    "        x = self.us4(x)\n",
    "        \n",
    "        x = self.last_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85364b0-7010-4710-9373-94a89db8d7c5",
   "metadata": {},
   "source": [
    "## Task 3: Create a training loop\n",
    "- split data into training and test data, e.g. 80% training data and 20% test data using your custom dataset.\n",
    "- Create a Dataloader for your custom datasets \n",
    "- Define a training loop for a single epoch:\n",
    "    - forward pass\n",
    "    - Loss function, e.g. cross entropy\n",
    "    - optimizer \n",
    "    - backward pass\n",
    "    - logging\n",
    "- Define validation loop:\n",
    "    - forward pass\n",
    "    - extract binary labels, e.g. threshold or argmax for each pixel.\n",
    "    - compute evaluation metrics: Accuracy, Precision, Recall and Intersection over Union for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72fb59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, \n",
    "    device,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    train_func,\n",
    "    test_func,\n",
    "    lr=0.001, \n",
    "    step_size=10, \n",
    "    gamma=0.1, \n",
    "    epochs=10,\n",
    "    **additional_params\n",
    "):\n",
    "    train_params = additional_params.pop(\"train_params\", {})\n",
    "    test_params = additional_params.pop(\"test_params\", {})\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_func(model, device, train_loader, optimizer, epoch, **train_params)\n",
    "        test_loss = test_func(model, device, test_loader, epoch, **test_params)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2281fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segmentation_model(\n",
    "    model, \n",
    "    device, \n",
    "    train_loader, \n",
    "    optimizer, \n",
    "    epoch\n",
    "):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    total_pixel = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # load images and masks to gpu\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += F.cross_entropy(output, target, reduction=\"sum\").item()\n",
    "        total_pixel += target.numel()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            data_processed_so_far = batch_idx * len(data)\n",
    "            total_data = len(train_loader.dataset)\n",
    "            progress = 100. * batch_idx / len(train_loader)\n",
    "            \n",
    "            print(\"Train Epoch {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, data_processed_so_far, total_data, progress, loss.item()\n",
    "            ), end=\"\\r\")\n",
    "            \n",
    "    train_loss /= total_pixel\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_segmentation_model(\n",
    "    model, \n",
    "    device, \n",
    "    test_loader, \n",
    "    epoch\n",
    "):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    test_loss = 0.\n",
    "    total_pixel = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            # process target and predicition\n",
    "            pred = output.argmax(dim=1)\n",
    "            test_loss += F.cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            total_pixel += target.numel()\n",
    "            all_targets.append(target.flatten().cpu())\n",
    "            all_preds.append(pred.flatten().cpu())\n",
    "\n",
    "    # compute accuracy, precision, recall, IoU\n",
    "    test_loss /= total_pixel\n",
    "    y_true = torch.cat(all_targets)\n",
    "    y_pred = torch.cat(all_preds)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    IoU = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    print(\"Test Set (Epoch {}): Avg. Test-Loss: {:.2f}\\tAccuracy: {:.2f}\\tPrecision: {:.2f}\\tRecall: {:.2f}\\tIoU: {:.2f}\".format(\n",
    "        epoch, test_loss, accuracy, precision, recall, IoU, 80 * \" \"\n",
    "    ))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e18a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "in_channels = 3\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create training and train loader\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size)\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size)\n",
    "\n",
    "# run training\n",
    "print(f\"Training is running with: {device}\")\n",
    "unet_model = UNet(in_channels, num_classes).to(device)\n",
    "train_losses, test_losses = train_model(\n",
    "    unet_model,\n",
    "    device,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    train_segmentation_model,\n",
    "    test_segmentation_model,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b952ae6",
   "metadata": {},
   "source": [
    "Test Set (Epoch 1): Avg. Test-Loss: 0.26\tAccuracy: 0.93\tPrecision: 0.99\tRecall: 0.21\tIoU: 0.21 <br>\n",
    "Test Set (Epoch 2): Avg. Test-Loss: 0.09\tAccuracy: 0.99\tPrecision: 1.00\tRecall: 0.84\tIoU: 0.84 <br>\n",
    "Test Set (Epoch 3): Avg. Test-Loss: 0.05\tAccuracy: 0.99\tPrecision: 0.99\tRecall: 0.95\tIoU: 0.94 <br>\n",
    "Test Set (Epoch 4): Avg. Test-Loss: 0.03\tAccuracy: 1.00\tPrecision: 0.98\tRecall: 0.96\tIoU: 0.95 <br>\n",
    "Test Set (Epoch 5): Avg. Test-Loss: 0.03\tAccuracy: 1.00\tPrecision: 0.99\tRecall: 0.96\tIoU: 0.95 <br>\n",
    "Test Set (Epoch 6): Avg. Test-Loss: 0.02\tAccuracy: 1.00\tPrecision: 0.99\tRecall: 0.97\tIoU: 0.96 <br>\n",
    "Test Set (Epoch 7): Avg. Test-Loss: 0.02\tAccuracy: 1.00\tPrecision: 0.98\tRecall: 0.98\tIoU: 0.96 <br>\n",
    "Test Set (Epoch 8): Avg. Test-Loss: 0.02\tAccuracy: 1.00\tPrecision: 0.97\tRecall: 0.98\tIoU: 0.96 <br>\n",
    "Test Set (Epoch 9): Avg. Test-Loss: 0.02\tAccuracy: 1.00\tPrecision: 0.97\tRecall: 0.99\tIoU: 0.96 <br>\n",
    "Test Set (Epoch 10): Avg. Test-Loss: 0.02\tAccuracy: 1.00\tPrecision: 0.97\tRecall: 0.99\tIoU: 0.95 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e603ff-a32e-4d90-84d9-eba8570d6e63",
   "metadata": {},
   "source": [
    "## Task 4: Small Report of your model and training\n",
    "- visualize training and test error over each epoch\n",
    "- report the evaluation metrics of the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03e8ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (ds1): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (ds2): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (ds3): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (ds4): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (ds5): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (us1): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (us2): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (us3): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (us4): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (trans_conv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (trans_conv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (trans_conv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (trans_conv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (last_conv): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the loaded model was trained on kaggle\n",
    "model = UNet(in_channels, num_classes)\n",
    "model_path = \"models/unet_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "93d5bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparision(gt, pred):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(gt, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(pred, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Predicted Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "94dfc1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "imgs, gts = next(iter(test_loader)) # get first batch\n",
    "img, gt = imgs[idx].unsqueeze(dim=0), (gts[idx] > 0).squeeze(dim=0).long()\n",
    "output = model(img).argmax(dim=1).squeeze(dim=0)\n",
    "plot_comparision(gt, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11943d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1, epochs + 1)\n",
    "plt.plot(x, train_losses, label=\"Train Loss\")\n",
    "plt.plot(x, test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Avg. Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d051a",
   "metadata": {},
   "source": [
    "<img src=\"plots/train_vs_test_error.png\" alt=\"Train vs. Test Error\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d201fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set (Epoch 1): Avg. Test-Loss: 0.02\tAccuracy: 1.00\tPrecision: 0.97\tRecall: 0.99\tIoU: 0.95\n"
     ]
    }
   ],
   "source": [
    "test_loss = test_segmentation_model(model, device, test_loader, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803f4ca",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "As the results show the model converges and attains high accuracy as well as solid IoU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
